{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d0ab33",
   "metadata": {},
   "source": [
    "# Use Tempered SMC to Improve Exploration of MCMC Methods.\n",
    "Multimodal distributions are typically hard to sample from, in particular using energy based methods such as HMC,\n",
    "as you need high energy levels to escape a potential well.\n",
    "\n",
    "Tempered SMC helps with this by considering a sequence of\n",
    "distributions:\n",
    "\n",
    "$$\n",
    "p_{\\lambda_k}(x) \\propto p_0(x) \\exp(-\\lambda_k V(x))\n",
    "$$\n",
    "\n",
    "where the tempering parameter $\\lambda_k$ takes increasing values between $0$ and $1$. Tempered SMC will also particularly shine when the MCMC step\n",
    "is not well calibrated (too small step size, etc) like in the example below.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce2db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from jax.scipy.stats import multivariate_normal\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "\n",
    "import blackjax\n",
    "import blackjax.smc.resampling as resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c4a638",
   "metadata": {},
   "source": [
    "## Sampling From a Bimodal Potential\n",
    "\n",
    "### Experimental Setup\n",
    "\n",
    "We consider a prior distribution\n",
    "\n",
    "$$\n",
    "p_0(x) = \\mathcal{N}(x \\mid 0, 1)\n",
    "$$\n",
    "\n",
    "and a potential function\n",
    "\n",
    "$$\n",
    "V(x) = (x^2 - 1)^2\n",
    "$$\n",
    "\n",
    "This corresponds to the following distribution. We plot the resulting tempered density for 5 different values of $\\lambda_k$ : from $\\lambda_k =1$ which correponds to the original density to $\\lambda_k=0$. The lower the value of $\\lambda_k$ the easier it is for the sampler to jump between the modes of the posterior density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385e5218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def V(x):\n",
    "    return 5 * jnp.square(jnp.sum(x**2) - 1)\n",
    "\n",
    "\n",
    "def prior_log_prob(x):\n",
    "    d = x.shape[0]\n",
    "    return multivariate_normal.logpdf(x, jnp.zeros((d,)), jnp.eye(d))\n",
    "\n",
    "\n",
    "linspace = jnp.linspace(-2, 2, 5000).reshape(-1, 1)\n",
    "lambdas = jnp.linspace(0.0, 1.0, 5)\n",
    "prior_logvals = jnp.vectorize(prior_log_prob, signature=\"(d)->()\")(linspace)\n",
    "potential_vals = jnp.vectorize(V, signature=\"(d)->()\")(linspace)\n",
    "log_res = prior_logvals.reshape(1, -1) - jnp.expand_dims(\n",
    "    lambdas, 1\n",
    ") * potential_vals.reshape(1, -1)\n",
    "\n",
    "density = jnp.exp(log_res)\n",
    "normalizing_factor = jnp.sum(density, axis=1, keepdims=True) * (\n",
    "    linspace[1] - linspace[0]\n",
    ")\n",
    "density /= normalizing_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a2b5e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.plot(linspace.squeeze(), density.T)\n",
    "ax.legend(list(lambdas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_loop(rng_key, mcmc_kernel, initial_state, num_samples):\n",
    "    @jax.jit\n",
    "    def one_step(state, k):\n",
    "        state, _ = mcmc_kernel(k, state)\n",
    "        return state, state\n",
    "\n",
    "    keys = jax.random.split(rng_key, num_samples)\n",
    "    _, states = jax.lax.scan(one_step, initial_state, keys)\n",
    "\n",
    "    return states\n",
    "\n",
    "\n",
    "def full_logdensity(x):\n",
    "    return -V(x) + prior_log_prob(x)\n",
    "\n",
    "\n",
    "inv_mass_matrix = jnp.eye(1)\n",
    "n_samples = 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383ee9e5",
   "metadata": {},
   "source": [
    "### Sample with HMC\n",
    "\n",
    "We first try to sample from the posterior density using an HMC kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "hmc_parameters = dict(\n",
    "    step_size=1e-4, inverse_mass_matrix=inv_mass_matrix, num_integration_steps=50\n",
    ")\n",
    "\n",
    "hmc = blackjax.hmc(full_logdensity, **hmc_parameters)\n",
    "hmc_state = hmc.init(jnp.ones((1,)))\n",
    "hmc_samples = inference_loop(key, hmc.step, hmc_state, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365d0246",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "samples = np.array(hmc_samples.position[:, 0])\n",
    "_ = plt.hist(samples, bins=100, density=True)\n",
    "_ = plt.plot(linspace.squeeze(), density[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b894a22a",
   "metadata": {},
   "source": [
    "### Sample with NUTS\n",
    "\n",
    "We now use a NUTS kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77edfed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nuts_parameters = dict(step_size=1e-4, inverse_mass_matrix=inv_mass_matrix)\n",
    "\n",
    "nuts = blackjax.nuts(full_logdensity, **nuts_parameters)\n",
    "nuts_state = nuts.init(jnp.ones((1,)))\n",
    "nuts_samples = inference_loop(key, nuts.step, nuts_state, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cca2b2",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "samples = np.array(nuts_samples.position[:, 0])\n",
    "_ = plt.hist(samples, bins=100, density=True)\n",
    "_ = plt.plot(linspace.squeeze(), density[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4dd4ed",
   "metadata": {},
   "source": [
    "## Tempered SMC with HMC Kernel\n",
    "\n",
    "We now use the adaptive tempered SMC algorithm with an HMC kernel. We only take one HMC step before resampling. The algorithm is run until $\\lambda_k$ crosses the $\\lambda_k = 1$ limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affdd4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smc_inference_loop(rng_key, smc_kernel, initial_state):\n",
    "    \"\"\"Run the temepered SMC algorithm.\n",
    "\n",
    "    We run the adaptive algorithm until the tempering parameter lambda reaches the value\n",
    "    lambda=1.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def cond(carry):\n",
    "        i, state, _k = carry\n",
    "        return state.lmbda < 1\n",
    "\n",
    "    def one_step(carry):\n",
    "        i, state, k = carry\n",
    "        k, subk = jax.random.split(k, 2)\n",
    "        state, _ = smc_kernel(subk, state)\n",
    "        return i + 1, state, k\n",
    "\n",
    "    n_iter, final_state, _ = jax.lax.while_loop(\n",
    "        cond, one_step, (0, initial_state, rng_key)\n",
    "    )\n",
    "\n",
    "    return n_iter, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8965f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "loglikelihood = lambda x: -V(x)\n",
    "\n",
    "hmc_parameters = dict(\n",
    "    step_size=1e-4, inverse_mass_matrix=inv_mass_matrix, num_integration_steps=1\n",
    ")\n",
    "\n",
    "tempered = blackjax.adaptive_tempered_smc(\n",
    "    prior_log_prob,\n",
    "    loglikelihood,\n",
    "    blackjax.hmc.kernel(),\n",
    "    blackjax.hmc.init,\n",
    "    hmc_parameters,\n",
    "    resampling.systematic,\n",
    "    0.5,\n",
    "    num_mcmc_steps=1,\n",
    ")\n",
    "\n",
    "initial_smc_state = jax.random.multivariate_normal(\n",
    "    jax.random.PRNGKey(0), jnp.zeros([1]), jnp.eye(1), (n_samples,)\n",
    ")\n",
    "initial_smc_state = tempered.init(initial_smc_state)\n",
    "\n",
    "n_iter, smc_samples = smc_inference_loop(key, tempered.step, initial_smc_state)\n",
    "print(\"Number of steps in the adaptive algorithm: \", n_iter.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a61d1e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "samples = np.array(smc_samples.particles[:, 0])\n",
    "_ = plt.hist(samples, bins=100, density=True)\n",
    "_ = plt.plot(linspace.squeeze(), density[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df97fcd",
   "metadata": {},
   "source": [
    "## Sampling from the Rastrigin Potential\n",
    "\n",
    "### Experimental Setup\n",
    "\n",
    "We consider a prior distribution $p_0(x) = \\mathcal{N}(x \\mid 0_2, 2 I_2)$ and we want to sample from a Rastrigin type potential function $V(x) = -2 A + \\sum_{i=1}^2x_i^2 - A  \\cos(2 \\pi x_i)$ where we choose $A=10$. These potential functions are known to be particularly hard to sample.\n",
    "\n",
    "We plot the resulting tempered density for 5 different values of $\\lambda_k$: from $\\lambda_k =1$ which correponds to the original density to $\\lambda_k=0$. The lower the value of $\\lambda_k$ the easier it is to sampler from the posterior log-density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_log_prob(x):\n",
    "    d = x.shape[0]\n",
    "    return multivariate_normal.logpdf(x, jnp.zeros((d,)), 2 * jnp.eye(d))\n",
    "\n",
    "\n",
    "def V(x):\n",
    "    d = x.shape[-1]\n",
    "    res = -10 * d + jnp.sum(x**2 - 10 * jnp.cos(2 * jnp.pi * x), -1)\n",
    "    return res\n",
    "\n",
    "\n",
    "linspace = jnp.linspace(-5, 5, 5000).reshape(-1, 1)\n",
    "lambdas = jnp.linspace(0.0, 1.0, 5)\n",
    "potential_vals = jnp.vectorize(V, signature=\"(d)->()\")(linspace)\n",
    "log_res = jnp.expand_dims(lambdas, 1) * potential_vals.reshape(1, -1)\n",
    "\n",
    "density = jnp.exp(-log_res)\n",
    "normalizing_factor = jnp.sum(density, axis=1, keepdims=True) * (\n",
    "    linspace[1] - linspace[0]\n",
    ")\n",
    "density /= normalizing_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b612568",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.semilogy(linspace.squeeze(), density.T)\n",
    "ax.legend(list(lambdas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749c6c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_loop(rng_key, mcmc_kernel, initial_state, num_samples):\n",
    "    def one_step(state, k):\n",
    "        state, _ = mcmc_kernel(k, state)\n",
    "        return state, state\n",
    "\n",
    "    keys = jax.random.split(rng_key, num_samples)\n",
    "    _, states = jax.lax.scan(one_step, initial_state, keys)\n",
    "\n",
    "    return states\n",
    "\n",
    "\n",
    "inv_mass_matrix = jnp.eye(1)\n",
    "n_samples = 1_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f5a6dd",
   "metadata": {},
   "source": [
    "### HMC Sampler\n",
    "\n",
    "We first try to sample from the posterior density using an HMC kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f1b992",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "loglikelihood = lambda x: -V(x)\n",
    "\n",
    "hmc_parameters = dict(\n",
    "    step_size=1e-2, inverse_mass_matrix=inv_mass_matrix, num_integration_steps=50\n",
    ")\n",
    "\n",
    "hmc = blackjax.hmc(full_logdensity, **hmc_parameters)\n",
    "hmc_state = hmc.init(jnp.ones((1,)))\n",
    "hmc_samples = inference_loop(key, hmc.step, hmc_state, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a04295e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "samples = np.array(hmc_samples.position[:, 0])\n",
    "_ = plt.hist(samples, bins=100, density=True)\n",
    "_ = plt.plot(linspace.squeeze(), density[-1])\n",
    "_ = plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f6f121",
   "metadata": {},
   "source": [
    "### NUTS Sampler\n",
    "\n",
    "We do the same using a NUTS kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e19b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nuts_parameters = dict(step_size=1e-2, inverse_mass_matrix=inv_mass_matrix)\n",
    "\n",
    "nuts = blackjax.nuts(full_logdensity, **nuts_parameters)\n",
    "nuts_state = nuts.init(jnp.ones((1,)))\n",
    "nuts_samples = inference_loop(key, nuts.step, nuts_state, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd5b40",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "samples = np.array(nuts_samples.position[:, 0])\n",
    "_ = plt.hist(samples, bins=100, density=True)\n",
    "_ = plt.plot(linspace.squeeze(), density[-1])\n",
    "_ = plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ebb6e2",
   "metadata": {},
   "source": [
    "### Tempered SMC with HMC Kernel\n",
    "\n",
    "We now use the adaptive tempered SMC algorithm with an HMC kernel. We only take one HMC step before resampling. The algorithm is run until $\\lambda_k$ crosses the $\\lambda_k = 1$ limit.\n",
    "We correct the bias introduced by the (arbitrary) prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97302e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "loglikelihood = lambda x: -V(x)\n",
    "\n",
    "hmc_parameters = dict(\n",
    "    step_size=1e-2, inverse_mass_matrix=inv_mass_matrix, num_integration_steps=100\n",
    ")\n",
    "\n",
    "tempered = blackjax.adaptive_tempered_smc(\n",
    "    prior_log_prob,\n",
    "    loglikelihood,\n",
    "    blackjax.hmc,\n",
    "    hmc_parameters,\n",
    "    resampling.systematic,\n",
    "    0.75,\n",
    "    mcmc_iter=1,\n",
    ")\n",
    "\n",
    "initial_smc_state = jax.random.multivariate_normal(\n",
    "    jax.random.PRNGKey(0), jnp.zeros([1]), jnp.eye(1), (n_samples,)\n",
    ")\n",
    "initial_smc_state = tempered.init(initial_smc_state)\n",
    "\n",
    "n_iter, smc_samples = smc_inference_loop(key, tempered.step, initial_smc_state)\n",
    "print(\"Number of steps in the adaptive algorithm: \", n_iter.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19b7ff3",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "samples = np.array(smc_samples.particles[:, 0])\n",
    "_ = plt.hist(samples, bins=100, density=True)\n",
    "_ = plt.plot(linspace.squeeze(), density[-1])\n",
    "_ = plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b62f62",
   "metadata": {},
   "source": [
    "The tempered SMC algorithm with the HMC kernel clearly outperfoms the HMC and NUTS kernels alone."
   ]
  }
 ],
 "metadata": {
  "file_format": "mystnb",
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.14.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "mystnb": {
   "execution_timeout": 200
  },
  "source_map": [
   15,
   33,
   44,
   64,
   89,
   97,
   116,
   122,
   136,
   142,
   148,
   158,
   164,
   170,
   196,
   225,
   231,
   241,
   265,
   273,
   287,
   293,
   309,
   316,
   322,
   332,
   339,
   346,
   374,
   381
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}