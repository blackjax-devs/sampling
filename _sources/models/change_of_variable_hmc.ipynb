{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14dcd323",
   "metadata": {},
   "source": [
    "# Change of Variable in HMC\n",
    "\n",
    "\n",
    "**Rat tumor problem:** We have J certain kinds of rat tumor diseases. For each kind of tumor, we test $N_{j}$ people/animals and among those $y_{j}$ tested positive. Here we assume that $y_{j}$ is distrubuted with **Binom**($N_{i}$, $\\theta_{i}$). Our objective is to approximate $\\theta_{j}$ for each type of tumor.\n",
    "\n",
    "In particular we use following binomial hierarchical model where $y_{j}$ and $N_{j}$ are observed variables.\n",
    "\n",
    "```{math}\n",
    "\\begin{align}\n",
    "    y_{j} &\\sim \\text{Binom}(N_{j}, \\theta_{j}) \\label{eq:1} \\\\\n",
    "    \\theta_{j} &\\sim \\text{Beta}(a, b) \\label{eq:2} \\\\\n",
    "    p(a, b) &\\propto (a+b)^{-5/2}\n",
    "\\end{align}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e08d9b6",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 80)\n",
    "\n",
    "import blackjax\n",
    "import tensorflow_probability.substrates.jax as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "plt.rc(\"xtick\", labelsize=12)  # fontsize of the xtick labels\n",
    "plt.rc(\"ytick\", labelsize=12)  # fontsize of the tyick labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb68181",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# index of array is type of tumor and value shows number of total people tested.\n",
    "group_size = jnp.array(\n",
    "    [\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        19,\n",
    "        19,\n",
    "        19,\n",
    "        19,\n",
    "        18,\n",
    "        18,\n",
    "        17,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        19,\n",
    "        19,\n",
    "        18,\n",
    "        18,\n",
    "        25,\n",
    "        24,\n",
    "        23,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        10,\n",
    "        49,\n",
    "        19,\n",
    "        46,\n",
    "        27,\n",
    "        17,\n",
    "        49,\n",
    "        47,\n",
    "        20,\n",
    "        20,\n",
    "        13,\n",
    "        48,\n",
    "        50,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        48,\n",
    "        19,\n",
    "        19,\n",
    "        19,\n",
    "        22,\n",
    "        46,\n",
    "        49,\n",
    "        20,\n",
    "        20,\n",
    "        23,\n",
    "        19,\n",
    "        22,\n",
    "        20,\n",
    "        20,\n",
    "        20,\n",
    "        52,\n",
    "        46,\n",
    "        47,\n",
    "        24,\n",
    "        14,\n",
    "    ],\n",
    "    dtype=jnp.float32,\n",
    ")\n",
    "\n",
    "# index of array is type of tumor and value shows number of positve people.\n",
    "n_of_positives = jnp.array(\n",
    "    [\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        0,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        2,\n",
    "        2,\n",
    "        2,\n",
    "        2,\n",
    "        2,\n",
    "        2,\n",
    "        2,\n",
    "        2,\n",
    "        2,\n",
    "        1,\n",
    "        5,\n",
    "        2,\n",
    "        5,\n",
    "        3,\n",
    "        2,\n",
    "        7,\n",
    "        7,\n",
    "        3,\n",
    "        3,\n",
    "        2,\n",
    "        9,\n",
    "        10,\n",
    "        4,\n",
    "        4,\n",
    "        4,\n",
    "        4,\n",
    "        4,\n",
    "        4,\n",
    "        4,\n",
    "        10,\n",
    "        4,\n",
    "        4,\n",
    "        4,\n",
    "        5,\n",
    "        11,\n",
    "        12,\n",
    "        5,\n",
    "        5,\n",
    "        6,\n",
    "        5,\n",
    "        6,\n",
    "        6,\n",
    "        6,\n",
    "        6,\n",
    "        16,\n",
    "        15,\n",
    "        15,\n",
    "        9,\n",
    "        4,\n",
    "    ],\n",
    "    dtype=jnp.float32,\n",
    ")\n",
    "\n",
    "# number of different kind of rat tumors\n",
    "n_rat_tumors = len(group_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ce709",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 3))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(range(n_rat_tumors), n_of_positives)\n",
    "\n",
    "ax.set_xlabel(\"tumor type\", fontsize=12)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "plt.title(\"No. of positives for each tumor type\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b7f5bd",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.bar(range(n_rat_tumors), group_size)\n",
    "plt.title(\"Group size for each tumor type\", fontsize=14)\n",
    "\n",
    "ax.set_xlabel(\"tumor type\", fontsize=12)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea3c9f4",
   "metadata": {},
   "source": [
    "## Posterior Sampling\n",
    "\n",
    "Now we use Blackjax's NUTS algorithm to get posterior samples of $a$, $b$, and $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e22cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "params = namedtuple(\"model_params\", [\"a\", \"b\", \"thetas\"])\n",
    "\n",
    "\n",
    "def joint_logdensity(params):\n",
    "    # improper prior for a,b\n",
    "    logdensity_ab = jnp.log(jnp.power(params.a + params.b, -2.5))\n",
    "\n",
    "    # logdensity prior of theta\n",
    "    logdensity_thetas = tfd.Beta(params.a, params.b).log_prob(params.thetas).sum()\n",
    "\n",
    "    # loglikelihood of y\n",
    "    logdensity_y = jnp.sum(\n",
    "        tfd.Binomial(group_size, probs=params.thetas).log_prob(n_of_positives)\n",
    "    )\n",
    "\n",
    "    return logdensity_ab + logdensity_thetas + logdensity_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4147e6e",
   "metadata": {},
   "source": [
    "We take initial parameters from uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32608c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(0)\n",
    "n_params = n_rat_tumors + 2\n",
    "\n",
    "\n",
    "def init_param_fn(seed):\n",
    "    \"\"\"\n",
    "    initialize a, b & thetas\n",
    "    \"\"\"\n",
    "    key1, key2, key3 = jax.random.split(seed, 3)\n",
    "    return params(\n",
    "        a=tfd.Uniform(0, 3).sample(seed=key1),\n",
    "        b=tfd.Uniform(0, 3).sample(seed=key2),\n",
    "        thetas=tfd.Uniform(0, 1).sample(n_rat_tumors, key3),\n",
    "    )\n",
    "\n",
    "\n",
    "init_param = init_param_fn(rng_key)\n",
    "joint_logdensity(init_param)  # sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4c5609",
   "metadata": {},
   "source": [
    "Now we use blackjax's window adaption algorithm to get NUTS kernel and initial states. Window adaption algorithm will automatically configure `inverse_mass_matrix` and `step size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d92658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warmup = blackjax.window_adaptation(blackjax.nuts, joint_logdensity)\n",
    "\n",
    "# we use 4 chains for sampling\n",
    "n_chains = 4\n",
    "keys = jax.random.split(rng_key, n_chains)\n",
    "init_params = jax.vmap(init_param_fn)(keys)\n",
    "\n",
    "@jax.vmap\n",
    "def call_warmup(seed, param):\n",
    "    (initial_states, tuned_params), _ = warmup.run(seed, param, 1000)\n",
    "    return initial_states, tuned_params\n",
    "\n",
    "initial_states, tuned_params = jax.jit(call_warmup)(keys, init_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e840ee",
   "metadata": {},
   "source": [
    "Now we write inference loop for multiple chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7ec4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_loop_multiple_chains(\n",
    "    rng_key, initial_states, tuned_params, log_prob_fn, num_samples, num_chains\n",
    "):\n",
    "    step_fn = blackjax.nuts.kernel()\n",
    "\n",
    "    def kernel(key, state, **params):\n",
    "        return step_fn(key, state, log_prob_fn, **params)\n",
    "\n",
    "    def one_step(states, rng_key):\n",
    "        keys = jax.random.split(rng_key, num_chains)\n",
    "        states, infos = jax.vmap(kernel)(keys, states, **tuned_params)\n",
    "        return states, (states, infos)\n",
    "\n",
    "    keys = jax.random.split(rng_key, num_samples)\n",
    "    _, (states, infos) = jax.lax.scan(one_step, initial_states, keys)\n",
    "\n",
    "    return (states, infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf8e57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_samples = 1000\n",
    "states, infos = inference_loop_multiple_chains(\n",
    "    rng_key, initial_states, tuned_params, joint_logdensity, n_samples, n_chains\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccf7345",
   "metadata": {},
   "source": [
    "## Arviz Plots\n",
    "\n",
    "We have all our posterior samples stored in `states.position` dictionary and `infos` store additional information like acceptance probability, divergence, etc. Now, we can use certain diagnostics to judge if our MCMC samples are converged on stationary distribution. Some of widely diagnostics are trace plots, potential scale reduction factor (R hat), divergences, etc. `Arviz` library provides quicker ways to anaylze these diagnostics. We can use `arviz.summary()` and `arviz_plot_trace()`, but these functions take specific format (arviz's trace) as a input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca482f4c",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def arviz_trace_from_states(states, info, burn_in=0):\n",
    "    position = states.position\n",
    "    if isinstance(position, jnp.DeviceArray):  # if states.position is array of samples\n",
    "        position = dict(samples=position)\n",
    "    else:\n",
    "        try:\n",
    "            position = position._asdict()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    samples = {}\n",
    "    for param in position.keys():\n",
    "        ndims = len(position[param].shape)\n",
    "        if ndims >= 2:\n",
    "            samples[param] = jnp.swapaxes(position[param], 0, 1)[\n",
    "                :, burn_in:\n",
    "            ]  # swap n_samples and n_chains\n",
    "            divergence = jnp.swapaxes(info.is_divergent[burn_in:], 0, 1)\n",
    "\n",
    "        if ndims == 1:\n",
    "            divergence = info.is_divergent\n",
    "            samples[param] = position[param]\n",
    "\n",
    "    trace_posterior = az.convert_to_inference_data(samples)\n",
    "    trace_sample_stats = az.convert_to_inference_data(\n",
    "        {\"diverging\": divergence}, group=\"sample_stats\"\n",
    "    )\n",
    "    trace = az.concat(trace_posterior, trace_sample_stats)\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667979a0",
   "metadata": {
    "tags": [
     "output-scroll"
    ]
   },
   "outputs": [],
   "source": [
    "# make arviz trace from states\n",
    "trace = arviz_trace_from_states(states, infos)\n",
    "summ_df = az.summary(trace)\n",
    "summ_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2570ccd5",
   "metadata": {},
   "source": [
    "**r_hat** is showing measure of each chain is converged to stationary distribution. **r_hat** should be less than or equal to 1.01, here we get r_hat far from 1.01 for each latent sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d9e847",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "az.plot_trace(trace)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5618cd9a",
   "metadata": {},
   "source": [
    "Trace plots also looks terrible and does not seems to be converged! Also, black band shows that every sample is diverged from original distribution. So **what's wrong happeing here?**\n",
    "\n",
    "Well, it's related to support of latent variable. In HMC, the latent variable must be in an unconstrained space, but in above model `theta` is constrained in between 0 to 1. We can use change of variable trick to solve above problem\n",
    "\n",
    "## Change of Variable\n",
    "We can sample from logits which is in unconstrained space and in `joint_logdensity()` we can convert logits to theta by suitable bijector (sigmoid). We calculate jacobian (first order derivaive) of bijector to tranform one probability distribution to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6330774",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_fn = jax.nn.sigmoid\n",
    "log_jacobian_fn = lambda logit: jnp.log(jnp.abs(jnp.diag(jax.jacfwd(transform_fn)(logit))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3909737a",
   "metadata": {},
   "source": [
    "Alternatively, using the bijector class in `TFP` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b6177",
   "metadata": {},
   "outputs": [],
   "source": [
    "bij = tfb.Sigmoid()\n",
    "transform_fn = bij.forward\n",
    "log_jacobian_fn = bij.forward_log_det_jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc098bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = namedtuple(\"model_params\", [\"a\", \"b\", \"logits\"])\n",
    "\n",
    "def joint_logdensity_change_of_var(params):\n",
    "    # change of variable\n",
    "    thetas = transform_fn(params.logits)\n",
    "    log_det_jacob = jnp.sum(log_jacobian_fn(params.logits))\n",
    "\n",
    "    # improper prior for a,b\n",
    "    logdensity_ab = jnp.log(jnp.power(params.a + params.b, -2.5))\n",
    "\n",
    "    # logdensity prior of theta\n",
    "    logdensity_thetas = tfd.Beta(params.a, params.b).log_prob(thetas).sum()\n",
    "\n",
    "    # loglikelihood of y\n",
    "    logdensity_y = jnp.sum(\n",
    "        tfd.Binomial(group_size, probs=thetas).log_prob(n_of_positives)\n",
    "    )\n",
    "\n",
    "    return logdensity_ab + logdensity_thetas + logdensity_y + log_det_jacob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f657a279",
   "metadata": {},
   "source": [
    "except for the change of variable in `joint_logdensity()` function, everthing will remain same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd3af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key = jax.random.PRNGKey(0)\n",
    "\n",
    "\n",
    "def init_param_fn(seed):\n",
    "    \"\"\"\n",
    "    initialize a, b & logits\n",
    "    \"\"\"\n",
    "    key1, key2, key3 = jax.random.split(seed, 3)\n",
    "    return params(\n",
    "        a=tfd.Uniform(0, 3).sample(seed=key1),\n",
    "        b=tfd.Uniform(0, 3).sample(seed=key2),\n",
    "        logits=tfd.Uniform(-2, 2).sample(n_rat_tumors, key3),\n",
    "    )\n",
    "\n",
    "\n",
    "init_param = init_param_fn(rng_key)\n",
    "joint_logdensity_change_of_var(init_param)  # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warmup = blackjax.window_adaptation(blackjax.nuts, joint_logdensity_change_of_var)\n",
    "\n",
    "# we use 4 chains for sampling\n",
    "n_chains = 4\n",
    "keys = jax.random.split(rng_key, n_chains)\n",
    "init_params = jax.vmap(init_param_fn)(keys)\n",
    "\n",
    "@jax.vmap\n",
    "def call_warmup(seed, param):\n",
    "    (initial_states, tuned_params), _ = warmup.run(seed, param, 1000)\n",
    "    return initial_states, tuned_params\n",
    "\n",
    "initial_states, tuned_params = call_warmup(keys, init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c7a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_samples = 1000\n",
    "states, infos = inference_loop_multiple_chains(\n",
    "    rng_key, initial_states, tuned_params, joint_logdensity_change_of_var, n_samples, n_chains\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06625939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert logits samples to theta samples\n",
    "position = states.position._asdict()\n",
    "position[\"thetas\"] = jax.nn.sigmoid(position[\"logits\"])\n",
    "del position[\"logits\"]  # delete logits\n",
    "states = states._replace(position=position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef0290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make arviz trace from states\n",
    "trace = arviz_trace_from_states(states, infos, burn_in=0)\n",
    "summ_df = az.summary(trace)\n",
    "summ_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49da5c3",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "az.plot_trace(trace)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa135c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of divergence: {infos.is_divergent.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb729942",
   "metadata": {},
   "source": [
    "We can see that **r_hat** is less than or equal to 1.01 for each latent variable, trace plots looks converged to stationary distribution, and only few samples are diverged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d7d7c4",
   "metadata": {},
   "source": [
    "## Using a PPL\n",
    "\n",
    "Probabilistic programming language usually provides functionality to apply change of variable easily (often done automatically). In this case for TFP, we can use its modeling API `tfd.JointDistribution*`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23342bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfed = tfp.experimental.distributions\n",
    "\n",
    "@tfd.JointDistributionCoroutineAutoBatched\n",
    "def model():\n",
    "    # TFP does not have improper prior, use uninformative prior instead\n",
    "    a = yield tfd.HalfCauchy(0, 100, name='a')\n",
    "    b = yield tfd.HalfCauchy(0, 100, name='b')\n",
    "    yield tfed.IncrementLogProb(jnp.log(jnp.power(a + b, -2.5)), name='logdensity_ab')\n",
    "\n",
    "    thetas = yield tfd.Sample(tfd.Beta(a, b), n_rat_tumors, name='thetas')\n",
    "    yield tfd.Binomial(group_size, probs=thetas, name='y')\n",
    "\n",
    "# Sample from the prior and prior predictive distributions. The result is a pytree.\n",
    "# model.sample(seed=rng_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa232c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition on the observed (and auxiliary variable).\n",
    "pinned = model.experimental_pin(logdensity_ab=(), y=n_of_positives)\n",
    "# Get the default change of variable bijectors from the model\n",
    "bijectors = pinned.experimental_default_event_space_bijector()\n",
    "\n",
    "prior_sample = pinned.sample_unpinned(seed=rng_key)\n",
    "# You can check the unbounded sample\n",
    "# bijectors.inverse(prior_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9bf048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_logdensity(unbound_param):\n",
    "    param = bijectors.forward(unbound_param)\n",
    "    log_det_jacobian = bijectors.forward_log_det_jacobian(unbound_param)\n",
    "    return pinned.unnormalized_log_prob(param) + log_det_jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1469e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rng_key = jax.random.PRNGKey(0)\n",
    "warmup = blackjax.window_adaptation(blackjax.nuts, joint_logdensity)\n",
    "\n",
    "# we use 4 chains for sampling\n",
    "n_chains = 4\n",
    "init_key, warmup_key = jax.random.split(rng_key, 2)\n",
    "init_params = bijectors.inverse(pinned.sample_unpinned(n_chains, seed=init_key))\n",
    "\n",
    "keys = jax.random.split(warmup_key, n_chains)\n",
    "\n",
    "@jax.vmap\n",
    "def call_warmup(seed, param):\n",
    "    (initial_states, tuned_params), _ = warmup.run(seed, param, 1000)\n",
    "    return initial_states, tuned_params\n",
    "\n",
    "initial_states, tuned_params = call_warmup(keys, init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd155c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_samples = 1000\n",
    "states, infos = inference_loop_multiple_chains(\n",
    "    rng_key, initial_states, tuned_params, joint_logdensity, n_samples, n_chains\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f7be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert logits samples to theta samples\n",
    "position = states.position\n",
    "states = states._replace(position=bijectors.forward(position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a7127",
   "metadata": {
    "tags": [
     "output-scroll"
    ]
   },
   "outputs": [],
   "source": [
    "# make arviz trace from states\n",
    "trace = arviz_trace_from_states(states, infos, burn_in=0)\n",
    "summ_df = az.summary(trace)\n",
    "summ_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f23864",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "az.plot_trace(trace)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "file_format": "mystnb",
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.14.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "mystnb": {
   "execution_timeout": 200
  },
  "source_map": [
   15,
   32,
   53,
   216,
   230,
   242,
   248,
   267,
   271,
   290,
   294,
   309,
   313,
   333,
   339,
   345,
   379,
   386,
   390,
   395,
   404,
   407,
   411,
   417,
   437,
   441,
   461,
   478,
   486,
   494,
   501,
   508,
   510,
   514,
   520,
   537,
   548,
   555,
   575,
   583,
   589,
   598
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}