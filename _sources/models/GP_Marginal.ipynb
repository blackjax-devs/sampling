{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbd02f3",
   "metadata": {},
   "source": [
    "# Bayesian Regression With Latent Gaussian Sampler\n",
    "\n",
    "In this example, we want to illustrate how to use the marginal sampler implementation [`mgrad_gaussian`](https://blackjax-devs.github.io/blackjax/mcmc.html#blackjax.mgrad_gaussian) of the article [Auxiliary gradient-based sampling algorithms](https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12269) {cite:p}`auxgradientalgo2018`. We do so by using the simulated data from the example [Gaussian Regression with the Elliptical Slice Sampler](https://blackjax-devs.github.io/blackjax/examples/GP_Elliauxgradientalgo2018pticalSliceSampler.html). Please also refer to the complementary example [Bayesian Logistic Regression With Latent Gaussian Sampler](https://blackjax-devs.github.io/blackjax/examples/LogisticRegressionWithLatentGaussianSampler.html).\n",
    "\n",
    "## Sampler Overview\n",
    "\n",
    "In section we give a brief overview of the idea behind this particular sampler. For more details please refer to the original paper [Auxiliary gradient-based sampling algorithms](https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12269) ([here](https://arxiv.org/abs/1610.09641) you can access the arXiv preprint).\n",
    "\n",
    "### Motivation: Auxiliary Metropolis-Hastings samplers\n",
    "\n",
    "Let us recall how to sample from a target density $\\pi(\\mathbf{x})$ using a Metropolis-Hasting sampler trough a *marginal scheme process*. The main idea is to have a mechanism that generate proposals $y$ which we then accept or reject according to a specific criterion. Concretely, suppose that we have an *auxiliary* scheme given by\n",
    "\n",
    "1. Sample $\\mathbf{u}|\\mathbf{x} \\sim \\pi(\\mathbf{u}|\\mathbf{x}) = q(\\mathbf{u}|\\mathbf{x})$.\n",
    "2. Generate proposal $\\mathbf{y}|\\mathbf{u}, \\mathbf{x} \\sim q(\\mathbf{y}|\\mathbf{x}, \\mathbf{u})$\n",
    "3. Compute the Metropolis-Hasting ratio\n",
    "\n",
    "```{math}\n",
    "\\tilde{\\varrho} = \\frac{\\pi(\\mathbf{y}|\\mathbf{u})q(\\mathbf{x}|\\mathbf{y}, \\mathbf{u})}{\\pi(\\mathbf{x}|\\mathbf{u})q(\\mathbf{y}|\\mathbf{x}, \\mathbf{u})}\n",
    "```\n",
    "\n",
    "4. Accept proposal $y$ with probability $\\min(1, \\tilde{\\varrho})$ and reject it otherwise.\n",
    "\n",
    "This scheme targets the auxiliary distribution $\\pi(\\mathbf{x}, \\mathbf{u}) = \\pi(\\mathbf{x}) q(\\mathbf{u}|\\mathbf{x})$ in two steps.\n",
    "\n",
    "Now, suppose we can instead compute the *marginal* proposal distribution $q(\\mathbf{y}|\\mathbf{x}) = \\int q(\\mathbf{y}|\\mathbf{x}, \\mathbf{u}) q(\\mathbf{u}|\\mathbf{x}) \\mathrm{d}u$ in closed form, then an alternative scheme is given by:\n",
    "\n",
    "1. We draw a proposal $y \\sim q(\\mathbf{y}\\mid\\mathbf{x})$.\n",
    "2. Then we compute the Metropolis-Hasting ratio\n",
    "\n",
    "```{math}\n",
    "\\varrho = \\frac{\\pi(\\mathbf{y})q(\\mathbf{x}|\\mathbf{y})}{\\pi(\\mathbf{x})q(\\mathbf{y}|\\mathbf{x})}\n",
    "```\n",
    "\n",
    "3. Accept proposal $y$ with probability $\\min(1, \\varrho)$ and reject it otherwise.\n",
    "\n",
    "### Example: Auxiliary Metropolis-Adjusted Langevin Algorithm (MALA)\n",
    "\n",
    "Let's consider the case of an auxiliary random walk proposal $q(\\mathbf{u}|\\mathbf{x}) = N(\\mathbf{u}|\\mathbf{x}, (\\delta /2) \\mathbf{I})$ for $\\delta > 0$ as in [[Section 2.2]  Auxiliary gradient-based sampling algorithms](https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12269), it is shown that one can use a first order approximation to sample from the (intractable) $\\pi(\\mathbf{x}|\\mathbf{u})$ density by choosing\n",
    "\n",
    "```{math}\n",
    "q(\\mathbf{y}|\\mathbf{u}, \\mathbf{x}) \\propto N(\\mathbf{y}|\\mathbf{u} + (\\delta/2)\\nabla \\log \\pi(\\mathbf{x}), (\\delta/2) I).\n",
    "```\n",
    "\n",
    "The resulting marginal sampler can be shown to correspond to the Metropolis-adjusted Langevin algorithm (MALA) with\n",
    "\n",
    "```{math}\n",
    "q(\\mathbf{y}| \\mathbf{x}) = N(\\mathbf{y}|\\mathbf{x} + (\\delta/2)\\nabla \\log \\pi(\\mathbf{x}), \\delta I).\n",
    "```\n",
    "\n",
    "### Latent Gaussian Models\n",
    "\n",
    "A particular case of interest is the latent Gaussian model where the target density has the form\n",
    "\n",
    "```{math}\n",
    "\\pi(\\mathbf{x}) \\propto \\overbrace{\\exp\\{f(\\mathbf{x})\\}}^{\\text{likelihood}} \\underbrace{N(\\mathbf{x}|\\mathbf{0}, \\mathbf{C})}_{\\text{Gaussian Prior}}\n",
    "```\n",
    "\n",
    "In this case, instead of linearising the full log density $\\log \\pi(\\mathbf{x})$, we can linearise $f$ only, which, when combined with a random walk proposal $N(\\mathbf{u}|\\mathbf{x}, (\\delta /2) \\mathbf{I})$, recovers to the following auxiliary proposal\n",
    "\n",
    "```{math}\n",
    "q(\\mathbf{y}|\\mathbf{x}, \\mathbf{u}) \\propto N\\left(\\mathbf{y}|\\frac{2}{\\delta} \\mathbf{A}\\left(\\mathbf{u} + \\frac{\\delta}{2}\\nabla f(\\mathbf{x})\\right), \\mathbf{A}\\right),\n",
    "```\n",
    "\n",
    "where $\\mathbf{A} = \\delta / 2(\\mathbf{C} + (\\delta / 2)\\mathbf{I})^{-1}\\mathbf{C}$. The corresponding marginal density is\n",
    "\n",
    "```{math}\n",
    "q(\\mathbf{y}|\\mathbf{x}) \\propto N\\left(\\mathbf{y}|\\frac{2}{\\delta} \\mathbf{A}\\left(\\mathbf{x} + \\frac{\\delta}{2}\\nabla f(\\mathbf{x})\\right), \\frac{2}{\\delta}\\mathbf{A}^2 + \\mathbf{A}\\right).\n",
    "```\n",
    "\n",
    "Sampling from $\\pi(\\mathbf{x}, \\mathbf{u})$, and therefore from $\\pi(\\mathbf{x})$, is done via Hastings-within-Gibbs as above.\n",
    "\n",
    "A crucial point of this algorithm is the fact that $\\mathbf{A}$ can be precomputed and afterward modified cheaply when $\\delta$ varies. This makes it easy to calibrate the step-size $\\delta$ at low cost.\n",
    "\n",
    "---\n",
    "\n",
    "Now that we have a high-level understanding of the algorithm, let's see how to use it in `blackjax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a099d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrnd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from blackjax import mgrad_gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3311a4f",
   "metadata": {},
   "source": [
    "We generate data through a squared exponential kernel as in the example [Gaussian Regression with the Elliptical Slice Sampler](https://blackjax-devs.github.io/blackjax/examples/GP_EllipticalSliceSampler.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb10347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_exponential(x, y, length, scale):\n",
    "    dot_diff = jnp.dot(x, x) + jnp.dot(y, y) - 2 * jnp.dot(x, y)\n",
    "    return scale**2 * jnp.exp(-0.5 * dot_diff / length**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6670533",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, d = 2000, 2\n",
    "length, scale = 1.0, 1.0\n",
    "y_sd = 1.0\n",
    "\n",
    "# fake data\n",
    "rng = jrnd.PRNGKey(10)\n",
    "kX, kf, ky = jrnd.split(rng, 3)\n",
    "\n",
    "X = jrnd.uniform(kX, shape=(n, d))\n",
    "Sigma = jax.vmap(\n",
    "    lambda x: jax.vmap(lambda y: squared_exponential(x, y, length, scale))(X)\n",
    ")(X) + 1e-3 * jnp.eye(n)\n",
    "invSigma = jnp.linalg.inv(Sigma)\n",
    "f = jrnd.multivariate_normal(kf, jnp.zeros(n), Sigma)\n",
    "y = f + jrnd.normal(ky, shape=(n,)) * y_sd\n",
    "\n",
    "# conjugate results\n",
    "posterior_cov = jnp.linalg.inv(invSigma + 1 / y_sd**2 * jnp.eye(n))\n",
    "posterior_mean = jnp.dot(posterior_cov, y) * 1 / y_sd**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a59e1c5",
   "metadata": {},
   "source": [
    "Let's visualize the distribution of the vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b40b5b",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(np.array(y), bins=50, density=True)\n",
    "plt.xlabel(\"y\")\n",
    "plt.title(\"Histogram of data.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65b0628",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now we proceed to run the sampler. First, we set the sampler parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb0a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling parameters\n",
    "n_warm = 2000\n",
    "n_iter = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30c0576",
   "metadata": {},
   "source": [
    "Next, we define the the log-probability function. For this we need to set the log-likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61cca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "loglikelihood_fn = lambda f: -0.5 * jnp.dot(y - f, y - f) / y_sd**2\n",
    "logdensity_fn = lambda f: loglikelihood_fn(f) - 0.5 * jnp.dot(f @ invSigma, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980828c0",
   "metadata": {},
   "source": [
    "Now we are ready to initialize the sampler. The output is type is a `NamedTuple` with the following fields:\n",
    "\n",
    "```\n",
    "init:\n",
    "    A pure function which when called with the initial position and the\n",
    "    target density probability function will return the kernel's initial\n",
    "    state.\n",
    "\n",
    "step:\n",
    "    A pure function that takes a rng key, a state and possibly some\n",
    "    parameters and returns a new state and some information about the\n",
    "    transition.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "init, step = mgrad_gaussian(logdensity_fn=logdensity_fn, mean=jnp.zeros(n), covariance=Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ee7f0",
   "metadata": {},
   "source": [
    "We continue by setting the inference loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea63dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_loop(rng, init_state, kernel, n_iter):\n",
    "    keys = jrnd.split(rng, n_iter)\n",
    "\n",
    "    def step(state, key):\n",
    "        state, info = kernel(key, state)\n",
    "        return state, (state, info)\n",
    "\n",
    "    _, (states, info) = jax.lax.scan(step, init_state, keys)\n",
    "    return states, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cca18a",
   "metadata": {},
   "source": [
    "We are now ready to run the sampler! The only extra parameters in the `step` function is `delta`, which (as seen in the sampler description) corresponds (in a loose sense) to the step-size of MALA algorithm.\n",
    "\n",
    "``` {admonition} Adaptation\n",
    "Note that one can calibrate the `delta` parameter as described in the example [Bayesian Logistic Regression With Latent Gaussian Sampler](https://blackjax-devs.github.io/blackjax/examples/LogisticRegressionWithLatentGaussianSampler.html).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a176ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kernel = lambda key, x: step(rng_key=key, state=x, delta=0.5)\n",
    "initial_state = init(f)\n",
    "\n",
    "states, info = inference_loop(jrnd.PRNGKey(0), init(f), kernel, n_warm + n_iter)\n",
    "samples = states.position[n_warm:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878945e9",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "\n",
    "Finally we evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b43105",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_mean = jnp.mean((samples.mean(axis=0) - posterior_mean) ** 2)\n",
    "error_cov = jnp.mean((jnp.cov(samples, rowvar=False) - posterior_cov) ** 2)\n",
    "print(\n",
    "    f\"Mean squared error for the mean vector {error_mean} and covariance matrix {error_cov}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c42e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = jrnd.split(rng, 500)\n",
    "predictive = jax.vmap(lambda k, f: f + jrnd.normal(k, (n,)) * y_sd)(\n",
    "    keys, samples[-1000:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a893dc",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(np.array(y), bins=50, density=True)\n",
    "plt.hist(np.array(predictive.reshape(-1)), bins=50, density=True, alpha=0.8)\n",
    "plt.xlabel(\"y\")\n",
    "plt.title(\"Predictive distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992d832d",
   "metadata": {},
   "source": [
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "file_format": "mystnb",
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.14.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "mystnb": {
   "execution_timeout": 200
  },
  "source_map": [
   15,
   94,
   102,
   107,
   113,
   133,
   137,
   145,
   151,
   155,
   159,
   162,
   178,
   180,
   184,
   194,
   202,
   210,
   216,
   224,
   231,
   240
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}