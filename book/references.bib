@misc{bettencourt2021sparsity ,
  author = "Bettencourt, Michael",
  year = "2021",
  title = "Sparsity Blues",
  url = "https://betanalpha.github.io/assets/case_studies/modeling_sparsity.html",
}

@article{carvalho2010horseshoe,
  title={The horseshoe estimator for sparse signals},
  author={Carvalho, Carlos M and Polson, Nicholas G and Scott, James G},
  journal={Biometrika},
  volume={97},
  number={2},
  pages={465--480},
  year={2010},
  publisher={Oxford University Press}
}

@misc{dua2017machine ,
  author = "Dua, Dheeru and Graff, Casey",
  year = "2017",
  title = "{UCI} Machine Learning Repository",
  url = "http://archive.ics.uci.edu/ml",
  institution = "University of California, Irvine, School of Information and Computer Sciences"
}

@inproceedings{hoffman2022tuning,
  title={Tuning-Free Generalized Hamiltonian Monte Carlo},
  author={Hoffman, Matthew D and Sountsov, Pavel},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={7799--7813},
  year={2022},
  organization={PMLR}
}

@article{horowitz1991generalized,
  title={A generalized guided Monte Carlo algorithm},
  author={Horowitz, Alan M},
  journal={Physics Letters B},
  volume={268},
  number={2},
  pages={247--252},
  year={1991},
  publisher={Elsevier}
}

@book{murphy2023probabilistic,
  title={Probabilistic machine learning: Advanced topics},
  author={Murphy, Kevin P},
  year={2023},
  publisher={MIT Press}
}

@article{neal2020non,
  title={Non-reversibly updating a uniform [0, 1] value for Metropolis accept/reject decisions},
  author={Neal, Radford M},
  journal={arXiv preprint arXiv:2001.11950},
  year={2020}
}

@article{papaspiliopoulos2007general,
  title={A general framework for the parametrization of hierarchical models},
  author={Papaspiliopoulos, Omiros and Roberts, Gareth O and Sk{\"o}ld, Martin},
  journal={Statistical Science},
  pages={59--73},
  year={2007},
  publisher={JSTOR}
}

@inproceedings{zhang2019cyclical,
  title={Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning},
  author={Zhang, Ruqi and Li, Chunyuan and Zhang, Jianyi and Chen, Changyou and Wilson, Andrew Gordon},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{auxgradientalgo2018,
  author = {Titsias, Michalis K. and Papaspiliopoulos, Omiros},
  title = {Auxiliary gradient-based sampling algorithms},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {80},
  number = {4},
  pages = {749-767},
  keywords = {Latent Gaussian models, Machine learning, Markov chain Monte Carlo sampling, Peskun ordering, Scalability},
  doi = {https://doi.org/10.1111/rssb.12269},
  url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12269},
  eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12269},
  abstract = {Summary We introduce a new family of Markov chain Monte Carlo samplers that combine auxiliary variables, Gibbs sampling and Taylor expansions of the target density. Our approach permits the marginalization over the auxiliary variables, yielding marginal samplers, or the augmentation of the auxiliary variables, yielding auxiliary samplers. The well-known Metropolis-adjusted Langevin algorithm MALA and preconditioned Crank–Nicolson–Langevin algorithm pCNL are shown to be special cases. We prove that marginal samplers are superior in terms of asymptotic variance and demonstrate cases where they are slower in computing time compared with auxiliary samplers. In the context of latent Gaussian models we propose new auxiliary and marginal samplers whose implementation requires a single tuning parameter, which can be found automatically during the transient phase. Extensive experimentation shows that the increase in efficiency (measured as the effective sample size per unit of computing time) relative to (optimized implementations of) pCNL, elliptical slice sampling and MALA ranges from tenfold in binary classification problems to 25 fold in log-Gaussian Cox processes to 100 fold in Gaussian process regression, and it is on a par with Riemann manifold Hamiltonian Monte Carlo sampling in an example where that algorithm has the same complexity as the aforementioned algorithms. We explain this remarkable improvement in terms of the way that alternative samplers try to approximate the eigenvalues of the target. We introduce a novel Markov chain Monte Carlo sampling scheme for hyperparameter learning that builds on the auxiliary samplers. The MATLAB code for reproducing the experiments in the paper is publicly available and an on-line supplement to this paper contains additional experiments and implementation details.},
  year = {2018}
}

@article{Blanes_2014,
  doi = {10.1137/130932740},
  url = {https://doi.org/10.1137%2F130932740},
  year = 2014,
  month = {jan},
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
  volume = {36},
  number = {4},
  pages = {A1556--A1580},
  author = {Sergio Blanes and Fernando Casas and J. M. Sanz-Serna},
  title = {Numerical Integrators for the Hybrid Monte Carlo Method},
  journal = {{SIAM} Journal on Scientific Computing}
}

@misc{orbitalMCMC2021,
  doi = {10.48550/ARXIV.2010.08047},
  url = {https://arxiv.org/abs/2010.08047},
  author = {Neklyudov, Kirill and Welling, Max},
  keywords = {Machine Learning (cs.LG), Computation (stat.CO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Orbital MCMC},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{maskedautoregressiveflow2017,
  doi = {10.48550/ARXIV.1705.07057},
  url = {https://arxiv.org/abs/1705.07057},
  author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Masked Autoregressive Flow for Density Estimation},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ellipticalslicesampling2010,
  doi = {10.48550/ARXIV.1001.0175},
  url = {https://arxiv.org/abs/1001.0175},
  author = {Murray, Iain and Adams, Ryan Prescott and MacKay, David J. C.},
  keywords = {Computation (stat.CO), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Elliptical slice sampling},
  publisher = {arXiv},
  year = {2010},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{tronarp2022fenrir,
  title={Fenrir: Physics-Enhanced Regression for Initial Value Problems},
  author={Tronarp, Filip and Bosch, Nathanael and Hennig, Philipp},
  booktitle={International Conference on Machine Learning},
  pages={21776--21794},
  year={2022},
  organization={PMLR}
}

@inproceedings{kersting2020differentiable,
  title={Differentiable Likelihoods for Fast Inversion of 'Likelihood-Free' Dynamical Systems},
  author={Kersting, Hans and Kr{\"a}mer, Nicholas and Schiegg, Martin and Daniel, Christian and Tiemann, Michael and Hennig, Philipp},
  booktitle={International Conference on Machine Learning},
  pages={5198--5208},
  year={2020},
  organization={PMLR}
}

@inproceedings{kramer2022probabilistic,
  title={Probabilistic ODE Solutions in Millions of Dimensions},
  author={Kr{\"a}mer, Nicholas and Bosch, Nathanael and Schmidt, Jonathan and Hennig, Philipp},
  booktitle={International Conference on Machine Learning},
  pages={11634--11649},
  year={2022},
  organization={PMLR}
}

@article{kramer2021linear,
  title={Linear-Time Probabilistic Solutions of Boundary Value Problems},
  author={Kr{\"a}mer, Nicholas and Hennig, Philipp},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={11160--11171},
  year={2021}
}

@misc{zhang2022pathfinder,
  title={Pathfinder: Parallel quasi-Newton variational inference}, 
  author={Lu Zhang and Bob Carpenter and Andrew Gelman and Aki Vehtari},
  year={2022},
  eprint={2108.03782},
  archivePrefix={arXiv},
  primaryClass={stat.ML}
}